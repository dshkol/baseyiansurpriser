% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/surprise-core.R
\name{kl_divergence}
\alias{kl_divergence}
\title{Kullback-Leibler Divergence}
\usage{
kl_divergence(posterior, prior, base = 2)
}
\arguments{
\item{posterior}{Numeric vector of posterior probabilities}

\item{prior}{Numeric vector of prior probabilities (same length as posterior)}

\item{base}{Base of logarithm (default: 2 for bits)}
}
\value{
Numeric scalar: the KL-divergence value (always non-negative)
}
\description{
Computes the KL-divergence from prior to posterior distribution,
which measures "surprise" in the Bayesian framework.
}
\details{
KL-divergence is defined as:
\deqn{D_{KL}(P || Q) = \sum_i P_i \log(P_i / Q_i)}

where P is the posterior and Q is the prior. The divergence is 0 when
posterior equals prior (no surprise), and increases as they differ.

Zero probabilities are handled by excluding those terms (convention that
0 * log(0) = 0).
}
\examples{
# No surprise when prior equals posterior
kl_divergence(c(0.5, 0.5), c(0.5, 0.5))

# High surprise when distributions differ
kl_divergence(c(0.9, 0.1), c(0.5, 0.5))

# Maximum surprise when posterior is certain
kl_divergence(c(1.0, 0.0), c(0.5, 0.5))
}
